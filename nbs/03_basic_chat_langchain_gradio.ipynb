{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic_chat_langchain_gradio\n",
    "\n",
    "> A basic gradio chat bot using OpenAI API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp basic_chat_langchain_gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import jarvis.secrets\n",
    "import jarvis.basic_chat_gradio as basic_chat_gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import openai\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.cache import SQLiteCache\n",
    "# import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain.llm_cache = SQLiteCache(database_path=\"../.langchain.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the default environment API key for the OpenAI API python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all OpenAI model listing the root model and release date, sorted by release date with the most recent at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | eval: false\n",
    "# # | output: false\n",
    "# demo = basic_chat_gradio.create_and_run_basic_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "# #| eval: false\n",
    "# #| output: false\n",
    "# demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse_codeblock` is a temporary fix for gradio displaying code correctly inside of triple backticks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Add tiktoken token counter for streaming api. https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "- Add running total calculator to track tokens and $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def process_user_message(\n",
    "    user_message:str, #Most recent message from user.\n",
    "    history:list[list[str]] #Message history from gradio.Chatbot instance.\n",
    ")->tuple[str,list[list[str]]]:\n",
    "    \"\"\"Add user message to 'history' and clear message textbox.\"\"\"\n",
    "    print('process_user_message',user_message,history)\n",
    "    # if history is None: history = []\n",
    "    return \"\", history + [[user_message, None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def process_bot_message(\n",
    "    history:list[list[str]], #Message history from gradio.Chatbot instance.\n",
    "    system_msg:str, #Most recent message from user.\n",
    "    selected_model:str, #Selected OpenAI model for chat completion.\n",
    "    temperature:float=0., #How deterministic the model results are.  See OpenAI docs.\n",
    "    stream:bool=True, #Whether or not to stream the results or return all at once when the model has finished its response.\n",
    ")->list[list[str]]:\n",
    "    \"\"\"Send chat history with most recent user message to the chat completions API.\"\"\"\n",
    "    stream_results = ''\n",
    "    history[-1][1] = ''\n",
    "    # try:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=selected_model,\n",
    "        messages=[{'role':'system', 'content': system_msg}] + gradio_chat_history_to_openai_chat_completions_format(history),\n",
    "        temperature=temperature,\n",
    "        stream=stream  # this time, we set stream=True\n",
    "    )\n",
    "    if stream:\n",
    "        for chunk in response:\n",
    "            model_results.append(model_results)\n",
    "            # print(chunk)\n",
    "            # print(chunk.get('choices',[{}])[0].get('delta',{}).get('content',''),end='')\n",
    "            stream_results += chunk.get('choices',[{}])[0].get('delta',{}).get('content','')\n",
    "            # history[-1][1] = parse_codeblock(stream_results) #removed because gradio properly formats code blocks now\n",
    "            history[-1][1] = stream_results\n",
    "            yield history\n",
    "    else:\n",
    "        # print(response)\n",
    "        # print(response.get('choices',[{}])[0].get('message',{}).get('content',''))\n",
    "        # print(parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content','')))\n",
    "        # history[-1][1] = parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content','')) #gradio fixed code blocks natively\n",
    "        history[-1][1] = response.get('choices',[{}])[0].get('message',{}).get('content','')\n",
    "        # print(history)\n",
    "        yield history\n",
    "    # except:\n",
    "    #     pass\n",
    "    #     # print(history, system_msg, [{'role':'system', 'content': system_msg}] + history_to_chatgpt_format(history))\n",
    "\n",
    "    # print(response)\n",
    "    # print(stream_results)\n",
    "    # print(parse_codeblock(stream_results))\n",
    "    # print(history[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "with gr.Blocks(title='Jarvis - My ChatGPT') as demo:\n",
    "    with gr.Tab(label='Chat'):\n",
    "        chatbot = gr.Chatbot()\n",
    "        # chatbot.change(fn=lambda: None, scroll_to_output=True) #This also doesn't work. Autoscroll when chatbot in tab is known issue.\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=10):\n",
    "                msg = gr.Textbox()\n",
    "            with gr.Column(scale=1):\n",
    "                clear = gr.Button(\"Clear\")\n",
    "                submit = gr.Button(\"Send\")\n",
    "    with gr.Tab(label='Settings'):\n",
    "        system_msg = gr.Textbox(value='You are a helpful assistant.', info='This is the system message for OpenAI Chat APIs.')\n",
    "        model_selection = gr.Dropdown(choices=['gpt-3.5-turbo','gpt-4'],value='gpt-3.5-turbo',label='Select model: ')\n",
    "        temperature = gr.Slider(maximum=2., value=0., step=.1, label='Temperature', info='What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.',)\n",
    "        stream_results = gr.Checkbox(value=True, label='Stream Results', )\n",
    "\n",
    "    submit0_kwargs = dict(fn=process_user_message, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    submit1_kwargs = dict(fn=process_bot_message, inputs=[chatbot, system_msg, model_selection, temperature, stream_results], outputs=chatbot)\n",
    "    msg.submit(**submit0_kwargs).then(**submit1_kwargs)\n",
    "    clear.click(fn=lambda: (None, None), inputs=None, outputs=[chatbot,msg], queue=False)\n",
    "    submit.click(**submit0_kwargs).then(**submit1_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| output: false\n",
    "demo.queue(concurrency_count=2)\n",
    "demo.launch(server_name='0.0.0.0', inline=False)#,server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
