{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a (so far) failed attempt at getting langchain streaming working in gradio. Will revisit later...<br>\n",
    "Reference: https://python.langchain.com/en/latest/modules/models/llms/examples/streaming_llm.html?highlight=Streaming#how-to-stream-llm-and-chat-model-responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain_gradio_streaming\n",
    "\n",
    "> A basic gradio chat bot using OpenAI API's, langchain and gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp langchain_gradio_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import jarvis.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# import openai\n",
    "# import numpy as np\n",
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "# import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.cache import SQLiteCache\n",
    "# import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain.llm_cache = SQLiteCache(database_path=\"../.langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import CallbackManager, BaseCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary? For tracking callback\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the default environment API key for the OpenAI API python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all OpenAI model listing the root model and release date, sorted by release date with the most recent at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # *,\n",
    "    # verbose: bool = None,\n",
    "    # callback_manager: langchain.callbacks.base.BaseCallbackManager = None,\n",
    "    # client = None,\n",
    "    # model_name: str = 'gpt-3.5-turbo',\n",
    "    # temperature: float = 0.7,\n",
    "    # model_kwargs: Dict[str, Any] = None,\n",
    "    # openai_api_key: Optional[str] = None,\n",
    "    # request_timeout: int = 60,\n",
    "    # max_retries: int = 6,\n",
    "    # streaming: bool = False,\n",
    "    # n: int = 1,\n",
    "    # max_tokens: Optional[int] = None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, LLMResult\n",
    "import queue\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Streaming Callback**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason on_llm_new_token is the only callback function that actually gets called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingCallbackHandler(BaseCallbackHandler):\n",
    "    def __init__(self, token_callback_func=None, stream=False):\n",
    "        super().__init__()\n",
    "        self.token_callback_func = token_callback_func\n",
    "        self.stream = stream\n",
    "        \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        print('NEW TOKEN', kwargs)\n",
    "        if self.stream and self.token_callback_func is not None:\n",
    "            self.token_callback_func(token)\n",
    "        \n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        print('on_llm_end called')\n",
    "        if self.stream and self.token_callback_func is not None:\n",
    "            self.token_callback_func(None)\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        print('on_chain_end called')\n",
    "        if self.stream and self.token_callback_func is not None:\n",
    "            self.token_callback_func(None)\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_tool_end(self, output: str, **kwargs) -> None:\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "        print('on_tool_end called')\n",
    "        if self.stream and self.token_callback_func is not None:\n",
    "            self.token_callback_func(None)\n",
    "\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "\n",
    "    def on_text(self, text: str, **kwargs) -> None:\n",
    "        \"\"\"Run on arbitrary text.\"\"\"\n",
    "        print('on_text called')\n",
    "        if self.stream and self.token_callback_func is not None:\n",
    "            self.token_callback_func(None)\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs) -> None:\n",
    "        \"\"\"Run on agent end.\"\"\"\n",
    "        print('on_agent_finish called')\n",
    "        if self.stream and self.token_callback_func is not None:\n",
    "            self.token_callback_func(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "BNEW TOKEN {}\n",
      "ubblesNEW TOKEN {}\n",
      " danceNEW TOKEN {}\n",
      " andNEW TOKEN {}\n",
      " shineNEW TOKEN {}\n",
      ",\n",
      "NEW TOKEN {}\n",
      "SparkNEW TOKEN {}\n",
      "lingNEW TOKEN {}\n",
      " waterNEW TOKEN {}\n",
      " onNEW TOKEN {}\n",
      " myNEW TOKEN {}\n",
      " tongueNEW TOKEN {}\n",
      ",\n",
      "NEW TOKEN {}\n",
      "RefreshingNEW TOKEN {}\n",
      " andNEW TOKEN {}\n",
      " pureNEW TOKEN {}\n",
      ".NEW TOKEN {}\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name = 'gpt-3.5-turbo', streaming=True, \n",
    "                  callback_manager=CallbackManager(\n",
    "                      [StreamingCallbackHandler(token_callback_func=lambda o: print(o,end=''),stream=True)]\n",
    "                  ), verbose=True, temperature=0)\n",
    "resp = chat([HumanMessage(content=\"Write me a short haiku about sparkling water.\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bubbles dance and shine,\\nSparkling water on my tongue,\\nRefreshing and pure.', additional_kwargs={})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback To Generator Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackToGeneratorConvertor:\n",
    "    def __init__(self):\n",
    "        self.q = queue.Queue()\n",
    "        self.generator_active = True\n",
    "\n",
    "    def callback(self, data):\n",
    "        self.q.put(data)\n",
    "        if data is None:\n",
    "            self.q.put(None)\n",
    "\n",
    "    def generator(self):\n",
    "        self.generator_active = True\n",
    "        while self.generator_active:\n",
    "            try:\n",
    "                data = self.q.get(True,0.01)\n",
    "                if data is None:\n",
    "                    self.generator_active = False\n",
    "                    break\n",
    "                else:\n",
    "                    yield data\n",
    "            except queue.Empty:\n",
    "                pass\n",
    "                # print('queue empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_to_generator_convertor = CallbackToGeneratorConvertor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name = 'gpt-3.5-turbo', streaming=True, \n",
    "                  callback_manager=CallbackManager([StreamingCallbackHandler(token_callback_func=callback_to_generator_convertor.callback,\n",
    "                                                                             stream=True)]), verbose=True, temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bubbles dance and shine,\\nSparkling water on my tongue,\\nRefreshing and pure.', additional_kwargs={})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content=\"Write me a short haiku about sparkling water.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.model_name = 'gpt-3.5-turbo'\n",
    "\n",
    "# chat([HumanMessage(content=\"Write me a short haiku about sparkling water.\")])\n",
    "thread = threading.Thread(target=chat, args=([HumanMessage(content=\"Write me a short haiku about sparkling water.\")],))\n",
    "thread.start()\n",
    "\n",
    "# [print('x',o) for o in callback_to_generator_convertor.generator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_to_generator_convertor.q.get(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread.is_alive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will add this back once streaming gets fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Add tiktoken token counter for streaming api. https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "- Add running total calculator to track tokens and $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n",
      "NEW TOKEN {}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
