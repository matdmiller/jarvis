[
  {
    "objectID": "basic_chat_gradio.html",
    "href": "basic_chat_gradio.html",
    "title": "basic_chat_gradio",
    "section": "",
    "text": "# from langchain.cache import SQLiteCache\n# import langchain\nSetting the default environment API key for the OpenAI API python client.\nList of all OpenAI model listing the root model and release date, sorted by release date with the most recent at the top."
  },
  {
    "objectID": "basic_chat_gradio.html#completion-messages",
    "href": "basic_chat_gradio.html#completion-messages",
    "title": "basic_chat_gradio",
    "section": "Completion Messages",
    "text": "Completion Messages\nAn example set of chat messages formatted for chat completion. Example from https://platform.openai.com/docs/guides/chat/introduction:\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)\nWith corresponding example response (non-streaming). Example from https://platform.openai.com/docs/api-reference/chat/create\n{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21\n  }\n}\nAnd finally an example streaming implementation https://huggingface.co/spaces/ysharma/ChatGPTwithAPI/blob/main/app.py\nBelow is a simple example of using Chat completions\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\nprint(completion.choices[0].message.content)\n\nHello there! How may I assist you today?\n\n\nAnd here is a streaming example\n\n#streaming completion\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=[\n        {'role': 'user', 'content': \"write a short, single verse, poem about math\"}\n    ],\n    temperature=0,\n    stream=True  # this time, we set stream=True\n)\n\nfor idx, chunk in enumerate(response):\n    if idx == 1: print(chunk)\n    print(chunk.get('choices',[{}])[0].get('delta',{}).get('content',''),end='')\n\n{\n  \"choices\": [\n    {\n      \"delta\": {\n        \"content\": \"Numbers\"\n      },\n      \"finish_reason\": null,\n      \"index\": 0\n    }\n  ],\n  \"created\": 1681622446,\n  \"id\": \"chatcmpl-75p6caNCtr22uffycUOdGBAp4j8Nw\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion.chunk\"\n}\nNumbers dance in perfect time,\nEquations flow like sweetest rhyme,\nMathematics, pure and true,\nA world of wonder, ever anew."
  },
  {
    "objectID": "basic_chat_gradio.html#gradio-interface",
    "href": "basic_chat_gradio.html#gradio-interface",
    "title": "basic_chat_gradio",
    "section": "Gradio Interface",
    "text": "Gradio Interface\nparse_codeblock is a temporary fix for gradio displaying code correctly inside of triple backticks.\n\ndef parse_codeblock(text):\n    \"\"\"Fixes how code blocks are displayed in the gradio chat window. \n    Ref: https://github.com/gradio-app/gradio/issues/3531\"\"\"\n    lines = text.split(\"\\n\")\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            if line != \"```\":\n                lines[i] = f'<pre><code class=\"{lines[i][3:]}\">'\n            else:\n                lines[i] = '</code></pre>'\n        else:\n            if i > 0:\n                lines[i] = \"<br/>\" + line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n    return \"\".join(lines)\n\n\nsource\n\nparse_codeblock\n\n parse_codeblock (text)\n\nFixes how code blocks are displayed in the gradio chat window. Ref: https://github.com/gradio-app/gradio/issues/3531\nTODO: - Add tiktoken token counter for streaming api. https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb - Add running total calculator to track tokens and $.\n\ndef gradio_chat_history_to_openai_chat_completions_format(\n    history:list[list[dict]] #Gradio chat history.\n)->list[dict]:\n    \"\"\"Converts gradio chat history to OpenAI messages completions format.\"\"\"\n    chat_formatted_list = []\n    for message_pair in history:\n        chat_formatted_list.append({'role':'user', 'content': message_pair[0]})\n        if message_pair[1] is not None:\n            chat_formatted_list.append({'role':'assistant', 'content': message_pair[1]})\n    return chat_formatted_list\n\n\nsource\n\n\ngradio_chat_history_to_openai_chat_completions_format\n\n gradio_chat_history_to_openai_chat_completions_format\n                                                        (history:list[list\n                                                        [dict]])\n\nConverts gradio chat history to OpenAI messages completions format.\n\n\n\n\nType\nDetails\n\n\n\n\nhistory\nlist\nGradio chat history.\n\n\nReturns\nlist\n\n\n\n\n\ndef process_user_message(\n    user_message:str, #Most recent message from user.\n    history:list[list[str]] #Message history from gradio.Chatbot instance.\n)->tuple[str,list[list[str]]]:\n    \"\"\"Add user message to 'history' and clear message textbox.\"\"\"\n    print('process_user_message',user_message,history)\n    # if history is None: history = []\n    return \"\", history + [[user_message, None]]\n\n\nsource\n\n\nprocess_user_message\n\n process_user_message (user_message:str, history:list[list[str]])\n\nAdd user message to ‘history’ and clear message textbox.\n\n\n\n\nType\nDetails\n\n\n\n\nuser_message\nstr\nMost recent message from user.\n\n\nhistory\nlist\nMessage history from gradio.Chatbot instance.\n\n\nReturns\ntuple\n\n\n\n\n\ndef process_bot_message(\n    history:list[list[str]], #Message history from gradio.Chatbot instance.\n    system_msg:str, #Most recent message from user.\n    selected_model:str, #Selected OpenAI model for chat completion.\n    temperature:float=0., #How deterministic the model results are.  See OpenAI docs.\n    stream:bool=True, #Whether or not to stream the results or return all at once when the model has finished its response.\n)->list[list[str]]:\n    \"\"\"Send chat history with most recent user message to the chat completions API.\"\"\"\n    stream_results = ''\n    history[-1][1] = ''\n    # try:\n    response = openai.ChatCompletion.create(\n        model=selected_model,\n        messages=[{'role':'system', 'content': system_msg}] + gradio_chat_history_to_openai_chat_completions_format(history),\n        temperature=temperature,\n        stream=stream  # this time, we set stream=True\n    )\n    if stream:\n        for chunk in response:\n            model_results.append(model_results)\n            # print(chunk)\n            # print(chunk.get('choices',[{}])[0].get('delta',{}).get('content',''),end='')\n            stream_results += chunk.get('choices',[{}])[0].get('delta',{}).get('content','')\n            # history[-1][1] = parse_codeblock(stream_results) #removed because gradio properly formats code blocks now\n            history[-1][1] = stream_results\n            yield history\n    else:\n        # print(response)\n        # print(response.get('choices',[{}])[0].get('message',{}).get('content',''))\n        # print(parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content','')))\n        # history[-1][1] = parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content','')) #gradio fixed code blocks natively\n        history[-1][1] = response.get('choices',[{}])[0].get('message',{}).get('content','')\n        # print(history)\n        yield history\n    # except:\n    #     pass\n    #     # print(history, system_msg, [{'role':'system', 'content': system_msg}] + history_to_chatgpt_format(history))\n\n    # print(response)\n    # print(stream_results)\n    # print(parse_codeblock(stream_results))\n    # print(history[-1][1])\n\n\nsource\n\n\nprocess_bot_message\n\n process_bot_message (history:list[list[str]], system_msg:str,\n                      selected_model:str, temperature:float=0.0,\n                      stream:bool=True)\n\nSend chat history with most recent user message to the chat completions API.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhistory\nlist\n\nMessage history from gradio.Chatbot instance.\n\n\nsystem_msg\nstr\n\nMost recent message from user.\n\n\nselected_model\nstr\n\nSelected OpenAI model for chat completion.\n\n\ntemperature\nfloat\n0.0\nHow deterministic the model results are. See OpenAI docs.\n\n\nstream\nbool\nTrue\nWhether or not to stream the results or return all at once when the model has finished its response.\n\n\nReturns\nlist\n\n\n\n\n\n\nmodel_results = []\nwith gr.Blocks(title='Jarvis - My ChatGPT') as demo:\n    with gr.Tab(label='Chat'):\n        chatbot = gr.Chatbot()\n        # chatbot.change(fn=lambda: None, scroll_to_output=True) #This also doesn't work. Autoscroll when chatbot in tab is known issue.\n        with gr.Row():\n            with gr.Column(scale=10):\n                msg = gr.Textbox()\n            with gr.Column(scale=1):\n                clear = gr.Button(\"Clear\")\n                submit = gr.Button(\"Send\")\n    with gr.Tab(label='Settings'):\n        system_msg = gr.Textbox(value='You are a helpful assistant.', info='This is the system message for OpenAI Chat APIs.')\n        model_selection = gr.Dropdown(choices=['gpt-3.5-turbo','gpt-4'],value='gpt-3.5-turbo',label='Select model: ')\n        temperature = gr.Slider(maximum=2., value=0., step=.1, label='Temperature', info='What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.',)\n        stream_results = gr.Checkbox(value=True, label='Stream Results', )\n\n    submit0_kwargs = dict(fn=process_user_message, inputs=[msg, chatbot], outputs=[msg, chatbot])\n    submit1_kwargs = dict(fn=process_bot_message, inputs=[chatbot, system_msg, model_selection, temperature, stream_results], outputs=chatbot)\n    msg.submit(**submit0_kwargs).then(**submit1_kwargs)\n    clear.click(fn=lambda: (None, None), inputs=None, outputs=[chatbot,msg], queue=False)\n    submit.click(**submit0_kwargs).then(**submit1_kwargs)\n\n\ndemo.queue(concurrency_count=2)\ndemo.launch(server_name='0.0.0.0')#,server_port=7860)\n\n\ndemo.close()\n\nClosing server running on port: 7862\n\n\n\ngr.close_all()"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jarvis",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "jarvis",
    "section": "Install",
    "text": "Install\nThe preferred way to install the package is to clone it and then install it. You will likely find the included notebooks useful.\ngit clone https://github.com/matdmiller/jarvis\ncd jarvis\npip install -e '.[dev]'\nAlternatively you can pip install it from the repo directly.\npip install git+https://github.com/matdmiller/jarvis\nInstalling the package by name is NOT supported at this time.\n#pip install jarvis"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "jarvis",
    "section": "How to use",
    "text": "How to use\nUsage examples will be included as development progresses. For now your best bet is to clone the repo and run the included notebooks in the nbs directory.\nFill me in please! Don’t forget code examples:\n\nfrom jarvis.core import *\n\n\nassert foo() == None\n1+1\n\n2"
  },
  {
    "objectID": "index.html#todos",
    "href": "index.html#todos",
    "title": "jarvis",
    "section": "Todos",
    "text": "Todos\n\nCollect text\nScrape text\nClean text\nSplit text\nEmbed text\nQuery text\nInject queried text into prompt"
  },
  {
    "objectID": "secrets.html",
    "href": "secrets.html",
    "title": "secrets",
    "section": "",
    "text": "You can reference secrets by adding them to your systems environment variables or via adding a jarvis_secrets.py file to the root of the cloned directory.  An example jarvis_secrets.py file could look like: \nimport os\n\nOPENAI_API_KEY = 'abc123'\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\nPINECONE_API_KEY = 'abc123'\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n\nPINECONE_ENV = 'abc123'\nos.environ[\"PINECONE_ENV\"] = PINECONE_ENV\n\nPINECONE_TEST_NAMESPACE = 'abc123'\nos.environ[\"PINECONE_TEST_NAMESPACE\"] = PINECONE_TEST_NAMESPACE\nImport sys and os to import the jarvis_secrets.py file in the repo root.\n\nimport sys, os\n\n\n#Check current directory - expected to be the notebook directory.\nprint(os.getcwd())\n#Check parent directory - should be the repo root directory which is where the jarvis_secrets.py file can be added for using your secrets.\nprint(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n\n/home/mathewmiller/ebs1/fastai_2022p2/jarvis/nbs\n/home/mathewmiller/ebs1/fastai_2022p2/jarvis\n\n\n\n# caution: path[0] is reserved for script path (or '' in REPL)\nsys.path.insert(1, os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n\n\ntry:\n    import jarvis_secrets\nexcept:\n    pass\n\nCheck to see if the test namespace was imported successfully.\n\nos.environ[\"PINECONE_TEST_NAMESPACE\"]\n\n'namespace_for_testing'\n\n\n\nsource\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "pinecone.html",
    "href": "pinecone.html",
    "title": "pinecone",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()\n\n\nEMBEDDING_LENGTH = 1536\n\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Pinecone\nfrom langchain.document_loaders import TextLoader\n\n\nfrom langchain.document_loaders import TextLoader\n\n\nloader = TextLoader('../examples/data/pg_essay_beyond_smart.txt')\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\ndocs = text_splitter.split_documents(documents)[:10] #limit length to 10 for this example.\nprint(len(docs))\n\nembeddings = OpenAIEmbeddings()\n\nCreated a chunk of size 549, which is longer than the specified 500\nCreated a chunk of size 667, which is longer than the specified 500\nCreated a chunk of size 767, which is longer than the specified 500\n\n\n10\n\n\n\nimport pinecone\n\n/opt/conda/envs/rapids/lib/python3.9/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\n# initialize pinecone\npinecone.init(\n    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n    environment=PINECONE_ENV  # next to api key in console\n)\n\nindex_name = \"pinecone-index-1\"\n\ndocsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name, namespace=PINECONE_TEST_NAMESPACE)\n\nquery = \"What are the keys to being smart?\"\ndocs = docsearch.similarity_search(query)\n\n\nprint(docs[0].page_content)\n\nWhy do so many smart people fail to discover anything new? Viewed from that direction, the question seems a rather depressing one. But there's another way to look at it that's not just more optimistic, but more interesting as well. Clearly intelligence is not the only ingredient in having new ideas. What are the other ingredients? Are they things we could cultivate?\n\n\n\nimport pinecone\n\n\npinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\nindex = pinecone.Index(\"pinecone-index-1\")\n\nquery_response = index.query(\n    namespace=PINECONE_TEST_NAMESPACE,\n    top_k=10,\n    include_values=True,\n    include_metadata=True,\n    vector=[0.1]*1536,\n    filter=None\n)\nprint(len(query_response.get('matches',[])))\n# query_response\n\n10\n\n\nPinecone query and response\nquery_response = index.query(\n    namespace='',\n    top_k=10,\n    include_values=True,\n    include_metadata=True,\n    vector=[0.1]*1536,\n    filter=None\n)\n\nRESPONSE:\n{'matches': [{'id': '263a129b-e46d-4102-a093-379575b45947',\n              'metadata': {'source': '../examples/data/pg_essay_beyond_smart.txt',\n                           'text': 'Notes\\n'\n                                   '\\n'\n                                   '[1] What wins in conversation depends on '\n                                   'who with. It ranges from mere '\n                                   'aggressiveness at the bottom, through '\n                                   'quick-wittedness in the middle, to '\n                                   'something closer to actual intelligence at '\n                                   'the top, though probably always with some '\n                                   'component of quick-wittedness.'},\n              'score': -0.0292761475,\n              'values': [-0.0114761228,\n                         0.0137850493,\n                         -0.0456852466]}],\n 'namespace': ''}\n\nids = []\nfor query_result_item in query_response.get('matches',[]):\n    ids.append(query_result_item['id'])\nif len(ids) > 0: index.delete(ids=ids, namespace=PINECONE_TEST_NAMESPACE)"
  },
  {
    "objectID": "openai_embeddings.html",
    "href": "openai_embeddings.html",
    "title": "openai_embeddings",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "langchain_gradio_streaming.html",
    "href": "langchain_gradio_streaming.html",
    "title": "langchain_gradio_streaming",
    "section": "",
    "text": "This is a (so far) failed attempt at getting langchain streaming working in gradio. Will revisit later…\nSetting the default environment API key for the OpenAI API python client.\nList of all OpenAI model listing the root model and release date, sorted by release date with the most recent at the top."
  },
  {
    "objectID": "langchain_gradio_streaming.html#streaming-callback",
    "href": "langchain_gradio_streaming.html#streaming-callback",
    "title": "langchain_gradio_streaming",
    "section": "Streaming Callback",
    "text": "Streaming Callback\nFor some reason on_llm_new_token is the only callback function that actually gets called.\n\nclass StreamingCallbackHandler(BaseCallbackHandler):\n    def __init__(self, token_callback_func=None, stream=False):\n        super().__init__()\n        self.token_callback_func = token_callback_func\n        self.stream = stream\n        \n    def on_llm_new_token(self, token: str, **kwargs):\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n        print('NEW TOKEN', kwargs)\n        if self.stream and self.token_callback_func is not None:\n            self.token_callback_func(token)\n        \n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs\n    ) -> None:\n        \"\"\"Run when LLM starts running.\"\"\"\n\n    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n        \"\"\"Run when LLM ends running.\"\"\"\n        print('on_llm_end called')\n        if self.stream and self.token_callback_func is not None:\n            self.token_callback_func(None)\n\n    def on_llm_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs\n    ) -> None:\n        \"\"\"Run when LLM errors.\"\"\"\n\n    def on_chain_start(\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs\n    ) -> None:\n        \"\"\"Run when chain starts running.\"\"\"\n\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n        \"\"\"Run when chain ends running.\"\"\"\n        print('on_chain_end called')\n        if self.stream and self.token_callback_func is not None:\n            self.token_callback_func(None)\n\n    def on_chain_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs\n    ) -> None:\n        \"\"\"Run when chain errors.\"\"\"\n\n    def on_tool_start(\n        self, serialized: Dict[str, Any], input_str: str, **kwargs\n    ) -> None:\n        \"\"\"Run when tool starts running.\"\"\"\n\n    def on_agent_action(self, action: AgentAction, **kwargs) -> Any:\n        \"\"\"Run on agent action.\"\"\"\n        pass\n\n    def on_tool_end(self, output: str, **kwargs) -> None:\n        \"\"\"Run when tool ends running.\"\"\"\n        print('on_tool_end called')\n        if self.stream and self.token_callback_func is not None:\n            self.token_callback_func(None)\n\n\n    def on_tool_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs\n    ) -> None:\n        \"\"\"Run when tool errors.\"\"\"\n\n    def on_text(self, text: str, **kwargs) -> None:\n        \"\"\"Run on arbitrary text.\"\"\"\n        print('on_text called')\n        if self.stream and self.token_callback_func is not None:\n            self.token_callback_func(None)\n\n    def on_agent_finish(self, finish: AgentFinish, **kwargs) -> None:\n        \"\"\"Run on agent end.\"\"\"\n        print('on_agent_finish called')\n        if self.stream and self.token_callback_func is not None:\n            self.token_callback_func(None)\n\n\nchat = ChatOpenAI(model_name = 'gpt-3.5-turbo', streaming=True, \n                  callback_manager=CallbackManager(\n                      [StreamingCallbackHandler(token_callback_func=lambda o: print(o,end=''),stream=True)]\n                  ), verbose=True, temperature=0)\nresp = chat([HumanMessage(content=\"Write me a short haiku about sparkling water.\")])\n\nNEW TOKEN {}\nNEW TOKEN {}\nBNEW TOKEN {}\nubblesNEW TOKEN {}\n danceNEW TOKEN {}\n andNEW TOKEN {}\n shineNEW TOKEN {}\n,\nNEW TOKEN {}\nSparkNEW TOKEN {}\nlingNEW TOKEN {}\n waterNEW TOKEN {}\n onNEW TOKEN {}\n myNEW TOKEN {}\n tongueNEW TOKEN {}\n,\nNEW TOKEN {}\nRefreshingNEW TOKEN {}\n andNEW TOKEN {}\n pureNEW TOKEN {}\n.NEW TOKEN {}\n\n\n\nresp\n\nAIMessage(content='Bubbles dance and shine,\\nSparkling water on my tongue,\\nRefreshing and pure.', additional_kwargs={})"
  },
  {
    "objectID": "langchain_gradio_streaming.html#callback-to-generator-wrapper",
    "href": "langchain_gradio_streaming.html#callback-to-generator-wrapper",
    "title": "langchain_gradio_streaming",
    "section": "Callback To Generator Wrapper",
    "text": "Callback To Generator Wrapper\n\nclass CallbackToGeneratorConvertor:\n    def __init__(self):\n        self.q = queue.Queue()\n        self.generator_active = True\n\n    def callback(self, data):\n        self.q.put(data)\n        if data is None:\n            self.q.put(None)\n\n    def generator(self):\n        self.generator_active = True\n        while self.generator_active:\n            try:\n                data = self.q.get(True,0.01)\n                if data is None:\n                    self.generator_active = False\n                    break\n                else:\n                    yield data\n            except queue.Empty:\n                pass\n                # print('queue empty')\n\n\ncallback_to_generator_convertor = CallbackToGeneratorConvertor()\n\n\nchat = ChatOpenAI(model_name = 'gpt-3.5-turbo', streaming=True, \n                  callback_manager=CallbackManager([StreamingCallbackHandler(token_callback_func=callback_to_generator_convertor.callback,\n                                                                             stream=True)]), verbose=True, temperature=0)\n\n\nchat([HumanMessage(content=\"Write me a short haiku about sparkling water.\")])\n\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\nNEW TOKEN {}\n\n\nAIMessage(content='Bubbles dance and shine,\\nSparkling water on my tongue,\\nRefreshing and pure.', additional_kwargs={})\n\n\n\nchat.model_name = 'gpt-3.5-turbo'\n\n# chat([HumanMessage(content=\"Write me a short haiku about sparkling water.\")])\nthread = threading.Thread(target=chat, args=([HumanMessage(content=\"Write me a short haiku about sparkling water.\")],))\nthread.start()\n\n# [print('x',o) for o in callback_to_generator_convertor.generator()]\n\n\ncallback_to_generator_convertor.q.get(False)\n\n''\n\n\n\nthread.is_alive()\n\nTrue"
  },
  {
    "objectID": "langchain_gradio_streaming.html#gradio-interface",
    "href": "langchain_gradio_streaming.html#gradio-interface",
    "title": "langchain_gradio_streaming",
    "section": "Gradio Interface",
    "text": "Gradio Interface\nWill add this back once streaming gets fixed.\nTODO: - Add tiktoken token counter for streaming api. https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb - Add running total calculator to track tokens and $."
  },
  {
    "objectID": "sqlite_explorer.html",
    "href": "sqlite_explorer.html",
    "title": "SQLite Explorer",
    "section": "",
    "text": "import sqlite3\n\n\nCollect text\nScrape text\nClean text\nSplit text\nEmbed text\nQuery text\nInject queried text into prompt\n\n\n\n# connect to the database\nconn = sqlite3.connect('../.langchain.db')\n# get a cursor object\ncursor = conn.cursor()\n# execute the SQL query\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n# fetch all the results\nresults = cursor.fetchall()\n# print the table names\nfor table in results:\n    print(table[0])\n\nfull_llm_cache\n\n\n\ncursor.execute(\"SELECT * FROM full_llm_cache;\")\n# fetch all the results\nresults = cursor.fetchall()\n# print the table names\nfor table in results:\n    print(table[0])"
  },
  {
    "objectID": "chromadb.html",
    "href": "chromadb.html",
    "title": "chromadb",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()\n\n\nEMBEDDING_LENGTH = 1536\n\nhttps://www.trychroma.com/  Langchain Docs: https://python.langchain.com/en/latest/reference/modules/vectorstore.html#langchain.vectorstores.Chroma  Langchain Examples: https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html \n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.document_loaders import TextLoader\n\n\nfrom langchain.document_loaders import TextLoader\n\n\nloader = TextLoader('../examples/data/pg_essay_beyond_smart.txt')\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\ndocs = text_splitter.split_documents(documents)#[:10] #limit length to 10 for this example.\nprint(len(docs))\n\nembeddings = OpenAIEmbeddings()\n\n11\n\n\n\ndb = Chroma.from_documents(docs, embeddings)\n\nUsing embedded DuckDB without persistence: data will be transient\n\n\n\nquery = \"What is the most important thing?\"\ndocs = db.similarity_search(query)\ndocs\n\n[Document(page_content=\"I grew up thinking that being smart was the thing most to be desired. Perhaps you did too. But I bet it's not what you really want. Imagine you had a choice between being really smart but discovering nothing new, and being less smart but discovering lots of new ideas. Surely you'd take the latter. I would. The choice makes me uncomfortable, but when you see the two options laid out explicitly like that, it's obvious which is better.\\n\\nThe reason the choice makes me uncomfortable is that being smart still feels like the thing that matters, even though I know intellectually that it isn't. I spent so many years thinking it was. The circumstances of childhood are a perfect storm for fostering this illusion. Intelligence is much easier to measure than the value of new ideas, and you're constantly being judged by it. Whereas even the kids who will ultimately discover new things aren't usually discovering them yet. For kids that way inclined, intelligence is the only game in town.\", metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'}),\n Document(page_content='That means the truth is excitingly different from the story I got as a kid. If intelligence is what matters, and also mostly inborn, the natural consequence is a sort of Brave New World fatalism. The best you can do is figure out what sort of work you have an \"aptitude\" for, so that whatever intelligence you were born with will at least be put to the best use, and then work as hard as you can at it. Whereas if intelligence isn\\'t what matters, but only one of several ingredients in what does, and many of those aren\\'t inborn, things get more interesting. You have a lot more control, but the problem of how to arrange your life becomes that much more complicated.\\n\\nSo what are the other ingredients in having new ideas? The fact that I can even ask this question proves the point I raised earlier — that society hasn\\'t assimilated the fact that it\\'s this and not intelligence that matters. Otherwise we\\'d all know the answers to such a fundamental question. [3]', metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'}),\n Document(page_content=\"I'm not going to try to provide a complete catalogue of the other ingredients here. This is the first time I've posed the question to myself this way, and I think it may take a while to answer. But I wrote recently about one of the most important: an obsessive interest in a particular topic. And this can definitely be cultivated.\\n\\nAnother quality you need in order to discover new ideas is independent-mindedness. I wouldn't want to claim that this is distinct from intelligence — I'd be reluctant to call someone smart who wasn't independent-minded — but though largely inborn, this quality seems to be something that can be cultivated to some extent.\", metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'}),\n Document(page_content=\"And of course there are a lot of fairly mundane ingredients in discovering new ideas, like working hard, getting enough sleep, avoiding certain kinds of stress, having the right colleagues, and finding tricks for working on what you want even when it's not what you're supposed to be working on. Anything that prevents people from doing great work has an inverse that helps them to. And this class of ingredients is not as boring as it might seem at first. For example, having new ideas is generally associated with youth. But perhaps it's not youth per se that yields new ideas, but specific things that come with youth, like good health and lack of responsibilities. Investigating this might lead to strategies that will help people of any age to have better ideas.\", metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'})]\n\n\n\nprint(docs[0].page_content)\nlen(docs[0].page_content)\n\nI grew up thinking that being smart was the thing most to be desired. Perhaps you did too. But I bet it's not what you really want. Imagine you had a choice between being really smart but discovering nothing new, and being less smart but discovering lots of new ideas. Surely you'd take the latter. I would. The choice makes me uncomfortable, but when you see the two options laid out explicitly like that, it's obvious which is better.\n\nThe reason the choice makes me uncomfortable is that being smart still feels like the thing that matters, even though I know intellectually that it isn't. I spent so many years thinking it was. The circumstances of childhood are a perfect storm for fostering this illusion. Intelligence is much easier to measure than the value of new ideas, and you're constantly being judged by it. Whereas even the kids who will ultimately discover new things aren't usually discovering them yet. For kids that way inclined, intelligence is the only game in town.\n\n\n987\n\n\n\ndb.similarity_search_with_score(query)\n\n[(Document(page_content=\"I grew up thinking that being smart was the thing most to be desired. Perhaps you did too. But I bet it's not what you really want. Imagine you had a choice between being really smart but discovering nothing new, and being less smart but discovering lots of new ideas. Surely you'd take the latter. I would. The choice makes me uncomfortable, but when you see the two options laid out explicitly like that, it's obvious which is better.\\n\\nThe reason the choice makes me uncomfortable is that being smart still feels like the thing that matters, even though I know intellectually that it isn't. I spent so many years thinking it was. The circumstances of childhood are a perfect storm for fostering this illusion. Intelligence is much easier to measure than the value of new ideas, and you're constantly being judged by it. Whereas even the kids who will ultimately discover new things aren't usually discovering them yet. For kids that way inclined, intelligence is the only game in town.\", metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'}),\n  0.4230814576148987),\n (Document(page_content='That means the truth is excitingly different from the story I got as a kid. If intelligence is what matters, and also mostly inborn, the natural consequence is a sort of Brave New World fatalism. The best you can do is figure out what sort of work you have an \"aptitude\" for, so that whatever intelligence you were born with will at least be put to the best use, and then work as hard as you can at it. Whereas if intelligence isn\\'t what matters, but only one of several ingredients in what does, and many of those aren\\'t inborn, things get more interesting. You have a lot more control, but the problem of how to arrange your life becomes that much more complicated.\\n\\nSo what are the other ingredients in having new ideas? The fact that I can even ask this question proves the point I raised earlier — that society hasn\\'t assimilated the fact that it\\'s this and not intelligence that matters. Otherwise we\\'d all know the answers to such a fundamental question. [3]', metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'}),\n  0.43470269441604614),\n (Document(page_content=\"I'm not going to try to provide a complete catalogue of the other ingredients here. This is the first time I've posed the question to myself this way, and I think it may take a while to answer. But I wrote recently about one of the most important: an obsessive interest in a particular topic. And this can definitely be cultivated.\\n\\nAnother quality you need in order to discover new ideas is independent-mindedness. I wouldn't want to claim that this is distinct from intelligence — I'd be reluctant to call someone smart who wasn't independent-minded — but though largely inborn, this quality seems to be something that can be cultivated to some extent.\", metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'}),\n  0.44913384318351746),\n (Document(page_content=\"And of course there are a lot of fairly mundane ingredients in discovering new ideas, like working hard, getting enough sleep, avoiding certain kinds of stress, having the right colleagues, and finding tricks for working on what you want even when it's not what you're supposed to be working on. Anything that prevents people from doing great work has an inverse that helps them to. And this class of ingredients is not as boring as it might seem at first. For example, having new ideas is generally associated with youth. But perhaps it's not youth per se that yields new ideas, but specific things that come with youth, like good health and lack of responsibilities. Investigating this might lead to strategies that will help people of any age to have better ideas.\", metadata={'source': '../examples/data/pg_essay_beyond_smart.txt'}),\n  0.45746248960494995)]"
  },
  {
    "objectID": "python_repl.html",
    "href": "python_repl.html",
    "title": "python_repl",
    "section": "",
    "text": "https://python.langchain.com/en/latest/modules/agents/tools/examples/python.html\n\nsource\n\nfoo\n\n foo ()\n\n\nfrom langchain.utilities import PythonREPL\n\n\npython_repl = PythonREPL()\n\n\npython_repl.run(\"print(1+1)\")\n\n'2\\n'"
  }
]