{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic_chat_gradio\n",
    "\n",
    "> A basic gradio chat bot using OpenAI API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp basic_chat_gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import jarvis.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import openai\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the default environment API key for the OpenAI API python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all OpenAI model listing the root model and release date, sorted by release date with the most recent at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4 - ROOT: gpt-4 - CREATED: 2023-03-12 07:03:22\n",
      "gpt-4-0314 - ROOT: gpt-4-0314 - CREATED: 2023-03-12 07:03:21\n",
      "gpt-3.5-turbo-0301 - ROOT: gpt-3.5-turbo-0301 - CREATED: 2023-03-01 05:52:43\n",
      "gpt-3.5-turbo - ROOT: gpt-3.5-turbo - CREATED: 2023-02-28 18:56:42\n",
      "whisper-1 - ROOT: whisper-1 - CREATED: 2023-02-27 21:13:04\n",
      "text-embedding-ada-002 - ROOT: text-embedding-ada-002 - CREATED: 2022-12-16 19:01:39\n",
      "text-davinci-003 - ROOT: text-davinci-003 - CREATED: 2022-11-28 01:40:35\n",
      "ada-code-search-text - ROOT: ada-code-search-text - CREATED: 2022-04-28 19:01:50\n",
      "babbage-search-document - ROOT: babbage-search-document - CREATED: 2022-04-28 19:01:50\n",
      "curie-similarity - ROOT: curie-similarity - CREATED: 2022-04-28 19:01:50\n",
      "babbage-code-search-code - ROOT: babbage-code-search-code - CREATED: 2022-04-28 19:01:49\n",
      "babbage-code-search-text - ROOT: babbage-code-search-text - CREATED: 2022-04-28 19:01:49\n",
      "davinci-search-document - ROOT: davinci-search-document - CREATED: 2022-04-28 19:01:49\n",
      "curie-search-query - ROOT: curie-search-query - CREATED: 2022-04-28 19:01:49\n",
      "text-search-curie-query-001 - ROOT: text-search-curie-query-001 - CREATED: 2022-04-28 19:01:49\n",
      "text-search-babbage-doc-001 - ROOT: text-search-babbage-doc-001 - CREATED: 2022-04-28 19:01:49\n",
      "text-search-curie-doc-001 - ROOT: text-search-curie-doc-001 - CREATED: 2022-04-28 19:01:49\n",
      "babbage-search-query - ROOT: babbage-search-query - CREATED: 2022-04-28 19:01:49\n",
      "text-search-babbage-query-001 - ROOT: text-search-babbage-query-001 - CREATED: 2022-04-28 19:01:49\n",
      "davinci-similarity - ROOT: davinci-similarity - CREATED: 2022-04-28 19:01:49\n",
      "curie-search-document - ROOT: curie-search-document - CREATED: 2022-04-28 19:01:48\n",
      "code-search-babbage-text-001 - ROOT: code-search-babbage-text-001 - CREATED: 2022-04-28 19:01:47\n",
      "code-search-babbage-code-001 - ROOT: code-search-babbage-code-001 - CREATED: 2022-04-28 19:01:47\n",
      "ada-similarity - ROOT: ada-similarity - CREATED: 2022-04-28 19:01:47\n",
      "code-search-ada-text-001 - ROOT: code-search-ada-text-001 - CREATED: 2022-04-28 19:01:47\n",
      "text-search-ada-doc-001 - ROOT: text-search-ada-doc-001 - CREATED: 2022-04-28 19:01:47\n",
      "text-similarity-curie-001 - ROOT: text-similarity-curie-001 - CREATED: 2022-04-28 19:01:47\n",
      "code-search-ada-code-001 - ROOT: code-search-ada-code-001 - CREATED: 2022-04-28 19:01:47\n",
      "ada-search-document - ROOT: ada-search-document - CREATED: 2022-04-28 19:01:47\n",
      "text-similarity-babbage-001 - ROOT: text-similarity-babbage-001 - CREATED: 2022-04-28 19:01:45\n",
      "babbage-similarity - ROOT: babbage-similarity - CREATED: 2022-04-28 19:01:45\n",
      "text-similarity-ada-001 - ROOT: text-similarity-ada-001 - CREATED: 2022-04-28 19:01:45\n",
      "ada-code-search-code - ROOT: ada-code-search-code - CREATED: 2022-04-28 19:01:45\n",
      "text-search-ada-query-001 - ROOT: text-search-ada-query-001 - CREATED: 2022-04-28 19:01:45\n",
      "ada-search-query - ROOT: ada-search-query - CREATED: 2022-04-28 19:01:45\n",
      "text-search-davinci-query-001 - ROOT: text-search-davinci-query-001 - CREATED: 2022-04-28 19:01:45\n",
      "davinci-search-query - ROOT: davinci-search-query - CREATED: 2022-04-28 19:01:45\n",
      "text-search-davinci-doc-001 - ROOT: text-search-davinci-doc-001 - CREATED: 2022-04-28 19:01:45\n",
      "text-similarity-davinci-001 - ROOT: text-similarity-davinci-001 - CREATED: 2022-04-28 19:01:45\n",
      "code-davinci-edit-001 - ROOT: code-davinci-edit-001 - CREATED: 2022-04-13 20:08:04\n",
      "text-davinci-002 - ROOT: text-davinci-002 - CREATED: 2022-04-13 20:08:04\n",
      "text-davinci-edit-001 - ROOT: text-davinci-edit-001 - CREATED: 2022-04-13 00:19:39\n",
      "text-curie-001 - ROOT: text-curie-001 - CREATED: 2022-04-07 20:40:43\n",
      "text-babbage-001 - ROOT: text-babbage-001 - CREATED: 2022-04-07 20:40:43\n",
      "text-davinci-001 - ROOT: text-davinci-001 - CREATED: 2022-04-07 20:40:42\n",
      "text-ada-001 - ROOT: text-ada-001 - CREATED: 2022-04-07 20:40:42\n",
      "curie-instruct-beta - ROOT: curie-instruct-beta - CREATED: 2022-04-07 20:40:42\n",
      "davinci-instruct-beta - ROOT: davinci-instruct-beta - CREATED: 2022-04-07 20:40:42\n",
      "davinci - ROOT: davinci - CREATED: 2022-04-07 19:31:14\n",
      "curie - ROOT: curie - CREATED: 2022-04-07 19:31:14\n",
      "babbage - ROOT: babbage - CREATED: 2022-04-07 19:07:29\n",
      "ada - ROOT: ada - CREATED: 2022-04-07 18:51:31\n",
      "text-babbage:001 - ROOT: text-babbage:001 - CREATED: 2022-01-12 20:12:50\n",
      "text-curie:001 - ROOT: text-curie:001 - CREATED: 2022-01-12 02:37:27\n",
      "text-ada:001 - ROOT: text-ada:001 - CREATED: 2022-01-12 01:06:48\n",
      "text-davinci:001 - ROOT: text-davinci:001 - CREATED: 2022-01-11 23:32:46\n",
      "davinci-instruct-beta:2.0.0 - ROOT: davinci-instruct-beta:2.0.0 - CREATED: 2021-08-20 23:25:14\n",
      "davinci-if:3.0.0 - ROOT: davinci-if:3.0.0 - CREATED: 2021-08-20 22:21:10\n",
      "if-davinci:3.0.0 - ROOT: if-davinci:3.0.0 - CREATED: 2021-08-20 00:52:35\n",
      "if-davinci-v2 - ROOT: if-davinci-v2 - CREATED: 2021-01-15 21:26:30\n",
      "if-curie-v2 - ROOT: if-curie-v2 - CREATED: 2021-01-15 21:26:08\n",
      "davinci:2020-05-03 - ROOT: davinci:2020-05-03 - CREATED: 2020-12-10 22:42:43\n",
      "curie:2020-05-03 - ROOT: curie:2020-05-03 - CREATED: 2020-12-10 20:38:45\n",
      "babbage:2020-05-03 - ROOT: babbage:2020-05-03 - CREATED: 2020-12-10 20:36:51\n",
      "ada:2020-05-03 - ROOT: ada:2020-05-03 - CREATED: 2020-12-10 20:20:25\n",
      "cushman:2020-05-03 - ROOT: cushman:2020-05-03 - CREATED: 2020-05-28 00:18:30\n"
     ]
    }
   ],
   "source": [
    "models = openai.Model.list()\n",
    "for _model in sorted(models.data, key=lambda o: o.get('created'), reverse=True):\n",
    "    print(f\"{_model.get('id')} - ROOT: {_model.get('root')} - CREATED: {datetime.datetime.fromtimestamp(_model.get('created'))}\")\n",
    "#models.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example set of chat messages formatted for chat completion. Example from https://platform.openai.com/docs/guides/chat/introduction:\n",
    "```python\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "With corresponding example response (non-streaming). Example from https://platform.openai.com/docs/api-reference/chat/create\n",
    "```python\n",
    "{\n",
    "  \"id\": \"chatcmpl-123\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1677652288,\n",
    "  \"choices\": [{\n",
    "    \"index\": 0,\n",
    "    \"message\": {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
    "    },\n",
    "    \"finish_reason\": \"stop\"\n",
    "  }],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 9,\n",
    "    \"completion_tokens\": 12,\n",
    "    \"total_tokens\": 21\n",
    "  }\n",
    "}\n",
    "```\n",
    "And finally an example streaming implementation https://huggingface.co/spaces/ysharma/ChatGPTwithAPI/blob/main/app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example of using Chat completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there, how can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a streaming example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"delta\": {\n",
      "        \"content\": \"Numbers\"\n",
      "      },\n",
      "      \"finish_reason\": null,\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1681257722,\n",
      "  \"id\": \"chatcmpl-74IDyxGMJpIhakCDEALergWPVDBg9\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion.chunk\"\n",
      "}\n",
      "Numbers dance in perfect time,\n",
      "Equations flow like sweetest rhyme,\n",
      "Mathematics, pure and true,\n",
      "A world of wonder, ever anew."
     ]
    }
   ],
   "source": [
    "#streaming completion\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"write a short, single verse, poem about math\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for idx, chunk in enumerate(response):\n",
    "    if idx == 1: print(chunk)\n",
    "    print(chunk.get('choices',[{}])[0].get('delta',{}).get('content',''),end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse_codeblock` is a temporary fix for gradio displaying code correctly inside of triple backticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def parse_codeblock(text):\n",
    "    \"\"\"Fixes how code blocks are displayed in the gradio chat window. \n",
    "    Ref: https://github.com/gradio-app/gradio/issues/3531\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            if line != \"```\":\n",
    "                lines[i] = f'<pre><code class=\"{lines[i][3:]}\">'\n",
    "            else:\n",
    "                lines[i] = '</code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                lines[i] = \"<br/>\" + line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "    return \"\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Add tiktoken token counter for streaming api. https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "- Add running total calculator to track tokens and $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def gradio_chat_history_to_openai_chat_completions_format(\n",
    "    history:list[list[dict]] #Gradio chat history.\n",
    ")->list[dict]:\n",
    "    \"\"\"Converts gradio chat history to OpenAI messages completions format.\"\"\"\n",
    "    chat_formatted_list = []\n",
    "    for message_pair in history:\n",
    "        chat_formatted_list.append({'role':'user', 'content': message_pair[0]})\n",
    "        if message_pair[1] is not None:\n",
    "            chat_formatted_list.append({'role':'assistant', 'content': message_pair[1]})\n",
    "    return chat_formatted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def process_user_message(\n",
    "    user_message:str, #Most recent message from user.\n",
    "    history:list[list[str]] #Message history from gradio.Chatbot instance.\n",
    ")->tuple[str,list[list[str]]]:\n",
    "    \"\"\"Add user message to 'history' and clear message textbox.\"\"\"\n",
    "    print('process_user_message',user_message,history)\n",
    "    # if history is None: history = []\n",
    "    return \"\", history + [[user_message, None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def process_bot_message(\n",
    "    history:list[list[str]], #Message history from gradio.Chatbot instance.\n",
    "    system_msg:str, #Most recent message from user.\n",
    "    selected_model:str, #Selected OpenAI model for chat completion.\n",
    "    temperature:float=0., #How deterministic the model results are.  See OpenAI docs.\n",
    "    stream:bool=True, #Whether or not to stream the results or return all at once when the model has finished its response.\n",
    ")->list[list[str]]:\n",
    "    \"\"\"Send chat history with most recent user message to the chat completions API.\"\"\"\n",
    "    stream_results = ''\n",
    "    history[-1][1] = ''\n",
    "    # try:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=selected_model,\n",
    "        messages=[{'role':'system', 'content': system_msg}] + gradio_chat_history_to_openai_chat_completions_format(history),\n",
    "        temperature=temperature,\n",
    "        stream=stream  # this time, we set stream=True\n",
    "    )\n",
    "    if stream:\n",
    "        for chunk in response:\n",
    "            model_results.append(model_results)\n",
    "            # print(chunk)\n",
    "            # print(chunk.get('choices',[{}])[0].get('delta',{}).get('content',''),end='')\n",
    "            stream_results += chunk.get('choices',[{}])[0].get('delta',{}).get('content','')\n",
    "            history[-1][1] = parse_codeblock(stream_results)\n",
    "            yield history\n",
    "    else:\n",
    "        # print(response)\n",
    "        # print(response.get('choices',[{}])[0].get('message',{}).get('content',''))\n",
    "        # print(parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content','')))\n",
    "        history[-1][1] = parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content',''))\n",
    "        # print(history)\n",
    "        yield history\n",
    "    # except:\n",
    "    #     pass\n",
    "    #     # print(history, system_msg, [{'role':'system', 'content': system_msg}] + history_to_chatgpt_format(history))\n",
    "\n",
    "    # print(response)\n",
    "    # print(stream_results)\n",
    "    # print(parse_codeblock(stream_results))\n",
    "    # print(history[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "with gr.Blocks(title='Jarvis - My ChatGPT') as demo:\n",
    "    with gr.Tab(label='Chat'):\n",
    "        chatbot = gr.Chatbot()\n",
    "        # chatbot.change(fn=lambda: None, scroll_to_output=True) #This also doesn't work. Autoscroll when chatbot in tab is known issue.\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=10):\n",
    "                msg = gr.Textbox()\n",
    "            with gr.Column(scale=1):\n",
    "                clear = gr.Button(\"Clear\")\n",
    "                submit = gr.Button(\"Send\")\n",
    "    with gr.Tab(label='Settings'):\n",
    "        system_msg = gr.Textbox(value='You are a helpful assistant.', info='This is the system message for OpenAI Chat APIs.')\n",
    "        model_selection = gr.Dropdown(choices=['gpt-3.5-turbo','gpt-4'],value='gpt-3.5-turbo',label='Select model: ')\n",
    "        temperature = gr.Slider(maximum=2., value=0., step=.1, label='Temperature', info='What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.',)\n",
    "        stream_results = gr.Checkbox(value=True, label='Stream Results', )\n",
    "\n",
    "    submit0_kwargs = dict(fn=process_user_message, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    submit1_kwargs = dict(fn=process_bot_message, inputs=[chatbot, system_msg, model_selection, temperature, stream_results], outputs=chatbot)\n",
    "    msg.submit(**submit0_kwargs).then(**submit1_kwargs)\n",
    "    clear.click(fn=lambda: (None, None), inputs=None, outputs=[chatbot,msg], queue=False)\n",
    "    submit.click(**submit0_kwargs).then(**submit1_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| output: false\n",
    "demo.queue(concurrency_count=2)\n",
    "demo.launch(server_name='0.0.0.0')#,server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
