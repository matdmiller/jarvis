# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_basic_chat_langchain_gradio.ipynb.

# %% auto 0
__all__ = ['process_user_message', 'process_bot_message']

# %% ../nbs/03_basic_chat_langchain_gradio.ipynb 4
import jarvis.secrets
import jarvis.basic_chat_gradio as basic_chat_gradio

# %% ../nbs/03_basic_chat_langchain_gradio.ipynb 5
import openai
import numpy as np
import os
import json
import gradio as gr
import datetime

# %% ../nbs/03_basic_chat_langchain_gradio.ipynb 19
def process_user_message(
    user_message:str, #Most recent message from user.
    history:list[list[str]] #Message history from gradio.Chatbot instance.
)->tuple[str,list[list[str]]]:
    """Add user message to 'history' and clear message textbox."""
    print('process_user_message',user_message,history)
    # if history is None: history = []
    return "", history + [[user_message, None]]

# %% ../nbs/03_basic_chat_langchain_gradio.ipynb 20
def process_bot_message(
    history:list[list[str]], #Message history from gradio.Chatbot instance.
    system_msg:str, #Most recent message from user.
    selected_model:str, #Selected OpenAI model for chat completion.
    temperature:float=0., #How deterministic the model results are.  See OpenAI docs.
    stream:bool=True, #Whether or not to stream the results or return all at once when the model has finished its response.
)->list[list[str]]:
    """Send chat history with most recent user message to the chat completions API."""
    stream_results = ''
    history[-1][1] = ''
    # try:
    response = openai.ChatCompletion.create(
        model=selected_model,
        messages=[{'role':'system', 'content': system_msg}] + gradio_chat_history_to_openai_chat_completions_format(history),
        temperature=temperature,
        stream=stream  # this time, we set stream=True
    )
    if stream:
        for chunk in response:
            model_results.append(model_results)
            # print(chunk)
            # print(chunk.get('choices',[{}])[0].get('delta',{}).get('content',''),end='')
            stream_results += chunk.get('choices',[{}])[0].get('delta',{}).get('content','')
            # history[-1][1] = parse_codeblock(stream_results) #removed because gradio properly formats code blocks now
            history[-1][1] = stream_results
            yield history
    else:
        # print(response)
        # print(response.get('choices',[{}])[0].get('message',{}).get('content',''))
        # print(parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content','')))
        # history[-1][1] = parse_codeblock(response.get('choices',[{}])[0].get('message',{}).get('content','')) #gradio fixed code blocks natively
        history[-1][1] = response.get('choices',[{}])[0].get('message',{}).get('content','')
        # print(history)
        yield history
    # except:
    #     pass
    #     # print(history, system_msg, [{'role':'system', 'content': system_msg}] + history_to_chatgpt_format(history))

    # print(response)
    # print(stream_results)
    # print(parse_codeblock(stream_results))
    # print(history[-1][1])
